{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplo de datos:\n",
      "    id                                               text        date  label\n",
      "0   1                       is so sad for my apl friend   04/03/2015      0\n",
      "1   2                         i miss the new moon trail   06/10/2015      0\n",
      "2   3                              omg it already 730 o   03/04/2015      1\n",
      "3   4   omgag im sooo im gunn cry i've been at thi de...  13/11/2015      0\n",
      "4   5                     i think mi bf is che on me tt   10/08/2015      0\n",
      "\n",
      "Tamaño del conjunto de entrenamiento: 1251424\n",
      "Tamaño del conjunto de prueba: 312856\n",
      "accuracy del modelo: 0.662576392973125\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from math import log\n",
    "\n",
    "# Función para cargar el dataset\n",
    "def cargar_dataset(ruta_csv, nrows):\n",
    "    # Saltar la primera fila usando skiprows=1\n",
    "    df = pd.read_csv(ruta_csv, sep=';', skiprows=1, header=None, names=[\"id\", \"text\", \"date\", \"label\"], low_memory=False, nrows=nrows)\n",
    "    \n",
    "    # Convertir la columna 'label' a tipo int\n",
    "    df[\"label\"] = pd.to_numeric(df[\"label\"], errors='coerce') \n",
    "    df = df.dropna(subset=[\"text\", \"label\"]).reset_index(drop=True)\n",
    "    df[\"label\"] = df[\"label\"].astype(int)\n",
    "    return df\n",
    "\n",
    "# División balanceada en train y test\n",
    "def dividir_train_test(df, test_size=0.2, seed=42):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    train_df, test_df = train_test_split(df, test_size=test_size, random_state=seed, stratify=df['label'])\n",
    "    return train_df, test_df\n",
    "\n",
    "# Construir diccionario y estadísticas\n",
    "def construir_diccionario(train_df, min_freq=1):\n",
    "    pos_texts = train_df[train_df['label'] == 1]['text']\n",
    "    neg_texts = train_df[train_df['label'] == 0]['text']\n",
    "\n",
    "    word_counts_pos = Counter()\n",
    "    word_counts_neg = Counter()\n",
    "\n",
    "    for text in pos_texts:\n",
    "        word_counts_pos.update(text.split())\n",
    "\n",
    "    for text in neg_texts:\n",
    "        word_counts_neg.update(text.split())\n",
    "\n",
    "    # Reducir diccionario eliminando palabras con frecuencia menor a min_freq (si lo necesitaras)\n",
    "    if min_freq > 1:\n",
    "        word_counts_pos = Counter({k: v for k, v in word_counts_pos.items() if v >= min_freq})\n",
    "        word_counts_neg = Counter({k: v for k, v in word_counts_neg.items() if v >= min_freq})\n",
    "\n",
    "    total_tweets = len(train_df)\n",
    "    p_pos = len(pos_texts) / total_tweets\n",
    "    p_neg = len(neg_texts) / total_tweets\n",
    "\n",
    "    return {\n",
    "        'word_counts_pos': word_counts_pos,\n",
    "        'word_counts_neg': word_counts_neg,\n",
    "        'p_pos': p_pos,\n",
    "        'p_neg': p_neg,\n",
    "        'vocab_size': len(set(word_counts_pos.keys()).union(set(word_counts_neg.keys())))\n",
    "    }\n",
    "\n",
    "# Clasificación de un tweet usando Naïve Bayes sin Laplace Smoothing\n",
    "def predecir_tweet_optimizado(tweet, stats):\n",
    "    log_prob_pos = log(stats['p_pos'])\n",
    "    log_prob_neg = log(stats['p_neg'])\n",
    "    vocab_size = stats['vocab_size']\n",
    "\n",
    "    words_pos = stats['word_counts_pos']\n",
    "    words_neg = stats['word_counts_neg']\n",
    "\n",
    "    total_words_pos = sum(words_pos.values())\n",
    "    total_words_neg = sum(words_neg.values())\n",
    "\n",
    "    words = tweet.split()\n",
    "\n",
    "    for word in words:\n",
    "        w_count_pos = words_pos.get(word, 0)\n",
    "        w_count_neg = words_neg.get(word, 0)\n",
    "\n",
    "        # Sin Laplace Smoothing:\n",
    "        # Si w_count_pos = 0 => P(w|pos) = 0 => log_prob_pos = -inf\n",
    "        if w_count_pos > 0:\n",
    "            log_prob_pos += log(w_count_pos / total_words_pos)\n",
    "        else:\n",
    "            log_prob_pos = float('-inf')\n",
    "\n",
    "        # Si w_count_neg = 0 => P(w|neg) = 0 => log_prob_neg = -inf\n",
    "        if w_count_neg > 0:\n",
    "            log_prob_neg += log(w_count_neg / total_words_neg)\n",
    "        else:\n",
    "            log_prob_neg = float('-inf')\n",
    "\n",
    "    return 1 if log_prob_pos > log_prob_neg else 0\n",
    "\n",
    "# Evaluación del modelo\n",
    "def evaluar_modelo_optimizado(test_df, stats):\n",
    "    y_pred = test_df['text'].apply(lambda tweet: predecir_tweet_optimizado(tweet, stats))\n",
    "    y_true = test_df['label'].values\n",
    "    accuracy = np.mean(y_pred == y_true)\n",
    "    print(\"accuracy del modelo:\", accuracy)\n",
    "    return accuracy\n",
    "\n",
    "# Main\n",
    "if __name__ == \"__main__\":\n",
    "    ruta_csv = \"./CRI_Practica3/FinalStemmedSentimentAnalysisDataset.csv\"\n",
    "    df = cargar_dataset(ruta_csv, None) # Segundo argumento = numero de filas a usar\n",
    "    print(\"Ejemplo de datos:\\n\", df.head())\n",
    "\n",
    "    # División en train y test\n",
    "    train_df, test_df = dividir_train_test(df)\n",
    "    print(f\"\\nTamaño del conjunto de entrenamiento: {len(train_df)}\")\n",
    "    print(f\"Tamaño del conjunto de prueba: {len(test_df)}\")\n",
    "\n",
    "    # Construir diccionario y estadísticas (sin Laplace)\n",
    "    stats = construir_diccionario(train_df)\n",
    "\n",
    "    # Evaluar el modelo (sin Laplace)\n",
    "    evaluar_modelo_optimizado(test_df, stats)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parte B: Estrategias (Sin Laplace Smoothing)\n",
      "\n",
      "Estrategia 1: Aumentar tamaño de entrenamiento (diccionario variable)\n",
      "Tamaño Train: 20000, accuracy: 0.5915\n",
      "Tamaño Train: 40000, accuracy: 0.5641\n",
      "Tamaño Train: 60000, accuracy: 0.5598\n",
      "Tamaño Train: 80000, accuracy: 0.5528\n",
      "\n",
      "Estrategia 2: Modificar tamaño del diccionario\n",
      "Frecuencia mínima: 1, accuracy: 0.5500\n",
      "Frecuencia mínima: 3, accuracy: 0.4971\n",
      "Frecuencia mínima: 5, accuracy: 0.4770\n",
      "Frecuencia mínima: 10, accuracy: 0.4587\n",
      "\n",
      "Estrategia 3: Modificar tamaño del conjunto de entrenamiento (diccionario fijo)\n",
      "Tamaño Train: 20000, accuracy: 0.4940\n",
      "Tamaño Train: 40000, accuracy: 0.5195\n",
      "Tamaño Train: 60000, accuracy: 0.5359\n",
      "Tamaño Train: 80000, accuracy: 0.5500\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluación del modelo\n",
    "def evaluar_modelo_optimizado(test_df, stats):\n",
    "    y_pred = test_df['text'].apply(lambda tweet: predecir_tweet_optimizado(tweet, stats))\n",
    "    y_true = test_df['label'].values\n",
    "    accuracy = np.mean(y_pred == y_true)\n",
    "    return accuracy\n",
    "\n",
    "# Estrategia 1: Aumentar el tamaño del conjunto de entrenamiento\n",
    "def estrategia_1(df, incrementos):\n",
    "    print(\"\\nEstrategia 1: Aumentar tamaño de entrenamiento (diccionario variable)\")\n",
    "    resultados = []\n",
    "    for n in incrementos:\n",
    "        train_df, test_df = dividir_train_test(df[:n])\n",
    "        stats = construir_diccionario(train_df)\n",
    "        acc = evaluar_modelo_optimizado(test_df, stats)\n",
    "        resultados.append((n, acc))\n",
    "        print(f\"Tamaño Train: {n}, accuracy: {acc:.4f}\")\n",
    "    return resultados\n",
    "\n",
    "# Estrategia 2: Reducir el diccionario manteniendo el train fijo\n",
    "def estrategia_2(df, min_freqs):\n",
    "    print(\"\\nEstrategia 2: Modificar tamaño del diccionario\")\n",
    "    train_df, test_df = dividir_train_test(df)\n",
    "    resultados = []\n",
    "    for min_freq in min_freqs:\n",
    "        stats = construir_diccionario(train_df, min_freq=min_freq)\n",
    "        acc = evaluar_modelo_optimizado(test_df, stats)\n",
    "        resultados.append((min_freq, acc))\n",
    "        print(f\"Frecuencia mínima: {min_freq}, accuracy: {acc:.4f}\")\n",
    "    return resultados\n",
    "\n",
    "# Estrategia 3: Mantener el tamaño del diccionario fijo y variar el tamaño del conjunto de entrenamiento\n",
    "def estrategia_3(df, train_sizes):\n",
    "    print(\"\\nEstrategia 3: Modificar tamaño del conjunto de entrenamiento (diccionario fijo)\")\n",
    "    # Dividir una sola vez para obtener test fijo\n",
    "    train_df, test_df = dividir_train_test(df, test_size=0.2)\n",
    "    # Construir diccionario con el train completo\n",
    "    stats_fijo = construir_diccionario(train_df)\n",
    "    resultados = []\n",
    "    for size in train_sizes:\n",
    "        train_subset = train_df[:size]\n",
    "        # Aquí se recalcula el diccionario con el subset, como en la versión original\n",
    "        stats = construir_diccionario(train_subset)\n",
    "        acc = evaluar_modelo_optimizado(test_df, stats)\n",
    "        resultados.append((size, acc))\n",
    "        print(f\"Tamaño Train: {size}, accuracy: {acc:.4f}\")\n",
    "    return resultados\n",
    "\n",
    "# Main\n",
    "if __name__ == \"__main__\":\n",
    "    ruta_csv = \"./CRI_Practica3/FinalStemmedSentimentAnalysisDataset.csv\"\n",
    "    print(\"Parte B: Estrategias (Sin Laplace Smoothing)\")\n",
    "    df = cargar_dataset(ruta_csv, 100000)\n",
    "\n",
    "    # Estrategia 1\n",
    "    estrategia_1(df, incrementos=[20000, 40000, 60000, 80000])\n",
    "\n",
    "    # Estrategia 2\n",
    "    estrategia_2(df, min_freqs=[1, 3, 5, 10])\n",
    "\n",
    "    # Estrategia 3\n",
    "    estrategia_3(df, train_sizes=[20000, 40000, 60000, 80000])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Estrategia 1 - Parte A]: Aumentar tamaño de entrenamiento con Laplace Smoothing\n",
      "Tamaño Train: 20000, alpha: 1, accuracy: 0.7522\n",
      "Tamaño Train: 40000, alpha: 1, accuracy: 0.7508\n",
      "Tamaño Train: 60000, alpha: 1, accuracy: 0.7614\n",
      "Tamaño Train: 80000, alpha: 1, accuracy: 0.7616\n",
      "\n",
      "[Estrategia 2 - Parte A]: Variar tamaño del diccionario con Laplace Smoothing (train fijo)\n",
      "Min. Frecuencia: 1, alpha: 1, accuracy: 0.7597\n",
      "Min. Frecuencia: 3, alpha: 1, accuracy: 0.7514\n",
      "Min. Frecuencia: 5, alpha: 1, accuracy: 0.7462\n",
      "\n",
      "[Estrategia 3 - Parte A]: Mantener diccionario fijo y variar tamaño de entrenamiento con Laplace Smoothing\n",
      "Tamaño Train: 20000, alpha: 1, accuracy: 0.7406\n",
      "Tamaño Train: 40000, alpha: 1, accuracy: 0.7533\n",
      "Tamaño Train: 60000, alpha: 1, accuracy: 0.7562\n",
      "Tamaño Train: 80000, alpha: 1, accuracy: 0.7597\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from math import log\n",
    "\n",
    "# Asumimos que ya tenemos las funciones cargar_dataset, dividir_train_test, construir_diccionario y predecir_tweet definidas.\n",
    "# También asumimos que predecir_tweet implementa Laplace Smoothing.\n",
    "\n",
    "def predecir_tweet(tweet, stats, alpha=1):\n",
    "    log_prob_pos = log(stats['p_pos'])\n",
    "    log_prob_neg = log(stats['p_neg'])\n",
    "    vocab_size = stats['vocab_size']\n",
    "    words = tweet.split()\n",
    "\n",
    "    words_pos = stats['word_counts_pos']\n",
    "    words_neg = stats['word_counts_neg']\n",
    "\n",
    "    total_words_pos = sum(words_pos.values())\n",
    "    total_words_neg = sum(words_neg.values())\n",
    "\n",
    "    for word in words:\n",
    "        p_w_pos = (words_pos.get(word, 0) + alpha) / (total_words_pos + alpha * vocab_size)\n",
    "        p_w_neg = (words_neg.get(word, 0) + alpha) / (total_words_neg + alpha * vocab_size)\n",
    "        log_prob_pos += log(p_w_pos)\n",
    "        log_prob_neg += log(p_w_neg)\n",
    "\n",
    "    return 1 if log_prob_pos > log_prob_neg else 0\n",
    "\n",
    "def evaluar_modelo(test_df, stats, alpha=1):\n",
    "    y_pred = test_df['text'].apply(lambda tweet: predecir_tweet(tweet, stats, alpha=alpha))\n",
    "    y_true = test_df['label'].values\n",
    "    return np.mean(y_pred == y_true)\n",
    "\n",
    "# Estrategia 1 con Laplace Smoothing: Aumentar el tamaño de entrenamiento\n",
    "def estrategia_1_laplace(df, incrementos, alpha=1):\n",
    "    print(\"\\n[Estrategia 1 - Parte A]: Aumentar tamaño de entrenamiento con Laplace Smoothing\")\n",
    "    for n in incrementos:\n",
    "        train_df, test_df = dividir_train_test(df[:n])\n",
    "        stats = construir_diccionario(train_df)\n",
    "        acc = evaluar_modelo(test_df, stats, alpha=alpha)\n",
    "        print(f\"Tamaño Train: {n}, alpha: {alpha}, accuracy: {acc:.4f}\")\n",
    "\n",
    "# Estrategia 2 con Laplace Smoothing: Fijar train, variar tamaño del diccionario\n",
    "def estrategia_2_laplace(df, min_freqs, alpha=1):\n",
    "    print(\"\\n[Estrategia 2 - Parte A]: Variar tamaño del diccionario con Laplace Smoothing (train fijo)\")\n",
    "    train_df, test_df = dividir_train_test(df)\n",
    "    for min_freq in min_freqs:\n",
    "        stats = construir_diccionario(train_df, min_freq=min_freq)\n",
    "        acc = evaluar_modelo(test_df, stats, alpha=alpha)\n",
    "        print(f\"Frecuencia mínima: {min_freq}, alpha: {alpha}, accuracy: {acc:.4f}\")\n",
    "\n",
    "# Estrategia 3 con Laplace Smoothing: Fijar diccionario, variar tamaño del entrenamiento\n",
    "def estrategia_3_laplace(df, train_sizes, alpha=1):\n",
    "    print(\"\\n[Estrategia 3 - Parte A]: Mantener diccionario fijo y variar tamaño de entrenamiento con Laplace Smoothing\")\n",
    "    # Primero creamos un conjunto de entrenamiento y test fijos\n",
    "    train_full_df, test_df = dividir_train_test(df)\n",
    "    # Creamos un diccionario con todo el train, que será nuestro diccionario base\n",
    "    stats_full = construir_diccionario(train_full_df)\n",
    "    full_vocab = set(stats_full['word_counts_pos'].keys()).union(set(stats_full['word_counts_neg'].keys()))\n",
    "    vocab_size = stats_full['vocab_size']\n",
    "\n",
    "    for size in train_sizes:\n",
    "        train_subset = train_full_df[:size]\n",
    "        # Recalculamos las frecuencias solo considerando las palabras del vocab completo\n",
    "        pos_texts = train_subset[train_subset['label'] == 1]['text']\n",
    "        neg_texts = train_subset[train_subset['label'] == 0]['text']\n",
    "        \n",
    "        word_counts_pos = Counter()\n",
    "        word_counts_neg = Counter()\n",
    "        \n",
    "        for text in pos_texts:\n",
    "            word_counts_pos.update([w for w in text.split() if w in full_vocab])\n",
    "        for text in neg_texts:\n",
    "            word_counts_neg.update([w for w in text.split() if w in full_vocab])\n",
    "        \n",
    "        p_pos = len(pos_texts) / len(train_subset)\n",
    "        p_neg = len(neg_texts) / len(train_subset)\n",
    "        \n",
    "        # Ahora creamos un stats con el vocab_size fijo\n",
    "        stats_subset = {\n",
    "            'word_counts_pos': word_counts_pos,\n",
    "            'word_counts_neg': word_counts_neg,\n",
    "            'p_pos': p_pos,\n",
    "            'p_neg': p_neg,\n",
    "            'vocab_size': vocab_size\n",
    "        }\n",
    "        \n",
    "        acc = evaluar_modelo(test_df, stats_subset, alpha=alpha)\n",
    "        print(f\"Tamaño Train: {size}, alpha: {alpha}, accuracy: {acc:.4f}\")\n",
    "\n",
    "# Main para la Parte A\n",
    "if __name__ == \"__main__\":\n",
    "    # Cargamos el dataset (definir cargar_dataset, dividir_train_test, construir_diccionario previamente)\n",
    "    ruta_csv = \"./CRI_Practica3/FinalStemmedSentimentAnalysisDataset.csv\"\n",
    "    df = cargar_dataset(ruta_csv, nrows=100000)\n",
    "    \n",
    "    # Parámetros para probar\n",
    "    incrementos = [20000, 40000, 60000, 80000]\n",
    "    min_freqs = [1, 3, 5]\n",
    "    train_sizes = [20000, 40000, 60000, 80000]\n",
    "    alpha = 1\n",
    "\n",
    "    # Estrategia 1 con Laplace\n",
    "    estrategia_1_laplace(df, incrementos, alpha=alpha)\n",
    "\n",
    "    # Estrategia 2 con Laplace\n",
    "    estrategia_2_laplace(df, min_freqs, alpha=alpha)\n",
    "\n",
    "    # Estrategia 3 con Laplace\n",
    "    estrategia_3_laplace(df, train_sizes, alpha=alpha)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
