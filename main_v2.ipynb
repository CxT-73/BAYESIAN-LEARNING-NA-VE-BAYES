{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplo de datos:\n",
      "    id                                               text        date  label\n",
      "0   1                       is so sad for my apl friend   04/03/2015      0\n",
      "1   2                         i miss the new moon trail   06/10/2015      0\n",
      "2   3                              omg it already 730 o   03/04/2015      1\n",
      "3   4   omgag im sooo im gunn cry i've been at thi de...  13/11/2015      0\n",
      "4   5                     i think mi bf is che on me tt   10/08/2015      0\n",
      "\n",
      "Tamaño del conjunto de entrenamiento: 79992\n",
      "Tamaño del conjunto de prueba: 19998\n",
      "accuracy del modelo: 0.7596759675967597\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from math import log\n",
    "\n",
    "# Función para cargar el dataset\n",
    "def cargar_dataset(ruta_csv, nrows=100000): #nrows=numfilas\n",
    "    # Saltar la primera fila usando skiprows=1\n",
    "    df = pd.read_csv(ruta_csv, sep=';', skiprows=1, header=None, names=[\"id\", \"text\", \"date\", \"label\"], low_memory=False, nrows=nrows) #,nrows=nrows\n",
    "    \n",
    "    # Convertir la columna 'label' a tipo int\n",
    "    df[\"label\"] = pd.to_numeric(df[\"label\"], errors='coerce')  # Coerce convierte valores no numéricos a NaN\n",
    "    \n",
    "    # Eliminar filas no válidas si las hay\n",
    "    df = df.dropna(subset=[\"text\", \"label\"]).reset_index(drop=True)\n",
    "    df[\"label\"] = df[\"label\"].astype(int)  # Convertir la columna 'label' a entero\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# División balanceada en train y test\n",
    "def dividir_train_test(df, test_size=0.2, seed=42):\n",
    "    # Estratificación por etiqueta\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    train_df, test_df = train_test_split(df, test_size=test_size, random_state=seed, stratify=df['label'])\n",
    "    return train_df, test_df\n",
    "\n",
    "# Construir diccionario y estadísticas\n",
    "def construir_diccionario(train_df, min_freq=1):\n",
    "    pos_texts = train_df[train_df['label'] == 1]['text']\n",
    "    neg_texts = train_df[train_df['label'] == 0]['text']\n",
    "\n",
    "    word_counts_pos = Counter()\n",
    "    word_counts_neg = Counter()\n",
    "\n",
    "    for text in pos_texts:\n",
    "        word_counts_pos.update(text.split())\n",
    "\n",
    "    for text in neg_texts:\n",
    "        word_counts_neg.update(text.split())\n",
    "\n",
    "    # Reducir diccionario eliminando palabras con frecuencia menor a min_freq\n",
    "    if min_freq > 1:\n",
    "        word_counts_pos = Counter({k: v for k, v in word_counts_pos.items() if v >= min_freq})\n",
    "        word_counts_neg = Counter({k: v for k, v in word_counts_neg.items() if v >= min_freq})\n",
    "\n",
    "    total_tweets = len(train_df)\n",
    "    p_pos = len(pos_texts) / total_tweets\n",
    "    p_neg = len(neg_texts) / total_tweets\n",
    "\n",
    "    return {\n",
    "        'word_counts_pos': word_counts_pos,\n",
    "        'word_counts_neg': word_counts_neg,\n",
    "        'p_pos': p_pos,\n",
    "        'p_neg': p_neg,\n",
    "        'vocab_size': len(set(word_counts_pos.keys()).union(set(word_counts_neg.keys())))\n",
    "    }\n",
    "\n",
    "\n",
    "# Clasificación de un tweet usando Naïve Bayes\n",
    "def predecir_tweet_optimizado(tweet, stats, alpha=1):\n",
    "    log_prob_pos = log(stats['p_pos'])\n",
    "    log_prob_neg = log(stats['p_neg'])\n",
    "    vocab_size = stats['vocab_size']\n",
    "\n",
    "    # Convertir los contadores en diccionarios para acceso rápido\n",
    "    words_pos = stats['word_counts_pos']\n",
    "    words_neg = stats['word_counts_neg']\n",
    "\n",
    "    # Precomputar totales\n",
    "    total_words_pos = sum(words_pos.values())\n",
    "    total_words_neg = sum(words_neg.values())\n",
    "\n",
    "    # Tokenización\n",
    "    words = tweet.split()\n",
    "\n",
    "    # Convertir en arrays para cálculos rápidos\n",
    "    word_probs_pos = np.array([log((words_pos.get(word, 0) + alpha) / (total_words_pos + alpha * vocab_size)) for word in words])\n",
    "    word_probs_neg = np.array([log((words_neg.get(word, 0) + alpha) / (total_words_neg + alpha * vocab_size)) for word in words])\n",
    "\n",
    "    # Sumar probabilidades en logaritmo\n",
    "    log_prob_pos += word_probs_pos.sum()\n",
    "    log_prob_neg += word_probs_neg.sum()\n",
    "\n",
    "    return 1 if log_prob_pos > log_prob_neg else 0\n",
    "\n",
    "# Evaluación del modelo\n",
    "def evaluar_modelo_optimizado(test_df, stats):\n",
    "    # Aplicar la predicción a cada tweet\n",
    "    y_pred = test_df['text'].apply(lambda tweet: predecir_tweet_optimizado(tweet, stats))\n",
    "    y_true = test_df['label'].values\n",
    "\n",
    "    # Calcular la accuracy\n",
    "    accuracy = np.mean(y_pred == y_true)\n",
    "    print(\"accuracy del modelo:\", accuracy)\n",
    "    return accuracy\n",
    "\n",
    "# Main\n",
    "if __name__ == \"__main__\":\n",
    "    ruta_csv = \"./CRI_Practica3/FinalStemmedSentimentAnalysisDataset.csv\"\n",
    "\n",
    "    # Cargar datos\n",
    "    df = cargar_dataset(ruta_csv)\n",
    "    print(\"Ejemplo de datos:\\n\", df.head())\n",
    "\n",
    "    # División en train y test\n",
    "    train_df, test_df = dividir_train_test(df)\n",
    "    print(f\"\\nTamaño del conjunto de entrenamiento: {len(train_df)}\")\n",
    "    print(f\"Tamaño del conjunto de prueba: {len(test_df)}\")\n",
    "\n",
    "    # Construir diccionario y estadísticas\n",
    "    stats = construir_diccionario(train_df)\n",
    "\n",
    "    # Evaluar el modelo\n",
    "    evaluar_modelo_optimizado(test_df, stats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Estrategia 1: Aumentar tamaño de entrenamiento (diccionario variable)\n",
      "Tamaño Train: 20000, accuracy: 0.7522\n",
      "Tamaño Train: 40000, accuracy: 0.7508\n",
      "Tamaño Train: 60000, accuracy: 0.7614\n",
      "Tamaño Train: 80000, accuracy: 0.7616\n",
      "\n",
      "Estrategia 2: Modificar tamaño del diccionario\n",
      "Min. Frecuencia: 1, accuracy: 0.7597\n",
      "Min. Frecuencia: 3, accuracy: 0.7514\n",
      "Min. Frecuencia: 5, accuracy: 0.7462\n",
      "Min. Frecuencia: 10, accuracy: 0.7386\n",
      "\n",
      "Estrategia 3: Modificar tamaño del conjunto de entrenamiento (diccionario fijo)\n",
      "Tamaño Train: 20000, accuracy: 0.7445\n",
      "Tamaño Train: 40000, accuracy: 0.7512\n",
      "Tamaño Train: 60000, accuracy: 0.7554\n",
      "Tamaño Train: 80000, accuracy: 0.7597\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from math import log\n",
    "\n",
    "# Clasificación de un tweet usando Naïve Bayes\n",
    "def predecir_tweet_optimizado(tweet, stats, alpha=1):\n",
    "    log_prob_pos = log(stats['p_pos'])\n",
    "    log_prob_neg = log(stats['p_neg'])\n",
    "    vocab_size = stats['vocab_size']\n",
    "\n",
    "    # Convertir los contadores en diccionarios para acceso rápido\n",
    "    words_pos = stats['word_counts_pos']\n",
    "    words_neg = stats['word_counts_neg']\n",
    "\n",
    "    # Precomputar totales\n",
    "    total_words_pos = sum(words_pos.values())\n",
    "    total_words_neg = sum(words_neg.values())\n",
    "\n",
    "    # Tokenización\n",
    "    words = tweet.split()\n",
    "\n",
    "    # Convertir en arrays para cálculos rápidos\n",
    "    word_probs_pos = np.array([log((words_pos.get(word, 0) + alpha) / (total_words_pos + alpha * vocab_size)) for word in words])\n",
    "    word_probs_neg = np.array([log((words_neg.get(word, 0) + alpha) / (total_words_neg + alpha * vocab_size)) for word in words])\n",
    "\n",
    "    # Sumar probabilidades en logaritmo\n",
    "    log_prob_pos += word_probs_pos.sum()\n",
    "    log_prob_neg += word_probs_neg.sum()\n",
    "\n",
    "    return 1 if log_prob_pos > log_prob_neg else 0\n",
    "\n",
    "def evaluar_modelo(test_df, stats):\n",
    "    y_true = test_df['label'].values\n",
    "    y_pred = [predecir_tweet_optimizado(row['text'], stats) for _, row in test_df.iterrows()]\n",
    "    return np.mean(y_pred == y_true)\n",
    "\n",
    "# Estrategia 1: Aumentar el tamaño del conjunto de entrenamiento\n",
    "def estrategia_1(df, incrementos):\n",
    "    print(\"\\nEstrategia 1: Aumentar tamaño de entrenamiento (diccionario variable)\")\n",
    "    resultados = []\n",
    "    for n in incrementos:\n",
    "        train_df, test_df = dividir_train_test(df[:n])\n",
    "        stats = construir_diccionario(train_df)\n",
    "        acc = evaluar_modelo(test_df, stats)\n",
    "        resultados.append((n, acc))\n",
    "        print(f\"Tamaño Train: {n}, accuracy: {acc:.4f}\")\n",
    "    return resultados\n",
    "\n",
    "# Estrategia 2: Reducir el diccionario\n",
    "def estrategia_2(df, min_freqs):\n",
    "    print(\"\\nEstrategia 2: Modificar tamaño del diccionario\")\n",
    "    train_df, test_df = dividir_train_test(df)\n",
    "    resultados = []\n",
    "    for min_freq in min_freqs:\n",
    "        stats = construir_diccionario(train_df, min_freq=min_freq)\n",
    "        acc = evaluar_modelo(test_df, stats)\n",
    "        resultados.append((min_freq, acc))\n",
    "        print(f\"Min. Frecuencia: {min_freq}, accuracy: {acc:.4f}\")\n",
    "    return resultados\n",
    "\n",
    "# Estrategia 3: Mantener el tamaño del diccionario fijo y variar el tamaño del conjunto de entrenamiento\n",
    "def estrategia_3(df, train_sizes):\n",
    "    print(\"\\nEstrategia 3: Modificar tamaño del conjunto de entrenamiento (diccionario fijo)\")\n",
    "    # Dividimos una sola vez el dataset para obtener el conjunto de test fijo\n",
    "    train_df, test_df = dividir_train_test(df, test_size=0.2)    \n",
    "    # Construir el diccionario usando el conjunto completo de entrenamiento\n",
    "    stats_fijo = construir_diccionario(train_df)    \n",
    "    resultados = []\n",
    "    for size in train_sizes:\n",
    "        # Tomar una parte del conjunto de entrenamiento\n",
    "        train_subset = train_df[:size]\n",
    "        stats = construir_diccionario(train_subset)  # Diccionario con el mismo vocabulario base\n",
    "        acc = evaluar_modelo(test_df, stats)\n",
    "        resultados.append((size, acc))\n",
    "        print(f\"Tamaño Train: {size}, accuracy: {acc:.4f}\")\n",
    "    return resultados\n",
    "\n",
    "\n",
    "# Main\n",
    "if __name__ == \"__main__\":\n",
    "    ruta_csv = \"./CRI_Practica3/FinalStemmedSentimentAnalysisDataset.csv\"\n",
    "      # Trabajamos con un subconjunto pequeño\n",
    "\n",
    "    df = cargar_dataset(ruta_csv, 100000)\n",
    "\n",
    "    # Estrategia 1: Aumentar tamaño de entrenamiento\n",
    "    estrategia_1(df, incrementos=[20000, 40000, 60000, 80000])\n",
    "\n",
    "    # Estrategia 2: Modificar tamaño del diccionario\n",
    "    estrategia_2(df, min_freqs=[1, 3, 5, 10])\n",
    "\n",
    "    # Estrategia 3: Variar tamaño del conjunto de entrenamiento\n",
    "    estrategia_3(df, train_sizes=[20000, 40000, 60000, 80000])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
