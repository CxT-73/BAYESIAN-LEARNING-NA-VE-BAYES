{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplo de datos:\n",
      "    id                                               text        date  label\n",
      "0   1                       is so sad for my apl friend   04/03/2015      0\n",
      "1   2                         i miss the new moon trail   06/10/2015      0\n",
      "2   3                              omg it already 730 o   03/04/2015      1\n",
      "3   4   omgag im sooo im gunn cry i've been at thi de...  13/11/2015      0\n",
      "4   5                     i think mi bf is che on me tt   10/08/2015      0\n",
      "Fold 1, Accuracy: 0.6642\n",
      "Fold 2, Accuracy: 0.6662\n",
      "Fold 3, Accuracy: 0.6645\n",
      "Fold 4, Accuracy: 0.6657\n",
      "Fold 5, Accuracy: 0.6655\n",
      "Fold 6, Accuracy: 0.6647\n",
      "Fold 7, Accuracy: 0.6653\n",
      "Fold 8, Accuracy: 0.6669\n",
      "Fold 9, Accuracy: 0.6648\n",
      "Fold 10, Accuracy: 0.6645\n",
      "\n",
      "Resultados K-Fold (10 folds):\n",
      "Mejor fold: 8 con accuracy: 0.6669\n",
      "Accuracy promedio: 0.6652\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from math import log\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def cargar_dataset(ruta_csv, nrows):\n",
    "    df = pd.read_csv(ruta_csv, sep=';', skiprows=1, header=None, names=[\"id\", \"text\", \"date\", \"label\"], low_memory=False, nrows=nrows)\n",
    "    df[\"label\"] = pd.to_numeric(df[\"label\"], errors='coerce') \n",
    "    df = df.dropna(subset=[\"text\", \"label\"]).reset_index(drop=True)\n",
    "    df[\"label\"] = df[\"label\"].astype(int)\n",
    "    return df\n",
    "\n",
    "def construir_diccionario(train_df, min_freq=1):\n",
    "    pos_texts = train_df[train_df['label'] == 1]['text']\n",
    "    neg_texts = train_df[train_df['label'] == 0]['text']\n",
    "\n",
    "    word_counts_pos = Counter()\n",
    "    word_counts_neg = Counter()\n",
    "\n",
    "    for text in pos_texts:\n",
    "        word_counts_pos.update(text.split())\n",
    "\n",
    "    for text in neg_texts:\n",
    "        word_counts_neg.update(text.split())\n",
    "\n",
    "    if min_freq > 1:\n",
    "        word_counts_pos = Counter({k: v for k, v in word_counts_pos.items() if v >= min_freq})\n",
    "        word_counts_neg = Counter({k: v for k, v in word_counts_neg.items() if v >= min_freq})\n",
    "\n",
    "    total_tweets = len(train_df)\n",
    "    p_pos = len(pos_texts) / total_tweets\n",
    "    p_neg = len(neg_texts) / total_tweets\n",
    "\n",
    "    vocab = set(word_counts_pos.keys()).union(set(word_counts_neg.keys()))\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    stats = {\n",
    "        'word_counts_pos': word_counts_pos,\n",
    "        'word_counts_neg': word_counts_neg,\n",
    "        'p_pos': p_pos,\n",
    "        'p_neg': p_neg,\n",
    "        'vocab_size': vocab_size\n",
    "    }\n",
    "\n",
    "    # Precomputar log-probabilidades\n",
    "    total_words_pos = sum(word_counts_pos.values())\n",
    "    total_words_neg = sum(word_counts_neg.values())\n",
    "    stats['total_words_pos'] = total_words_pos\n",
    "    stats['total_words_neg'] = total_words_neg\n",
    "\n",
    "    log_prob_pos_dict = {w: log(count / total_words_pos) for w, count in word_counts_pos.items()}\n",
    "    log_prob_neg_dict = {w: log(count / total_words_neg) for w, count in word_counts_neg.items()}\n",
    "\n",
    "    stats['log_prob_pos_dict'] = log_prob_pos_dict\n",
    "    stats['log_prob_neg_dict'] = log_prob_neg_dict\n",
    "\n",
    "    return stats\n",
    "\n",
    "def predecir_tweet_optimizado(tweet, stats):\n",
    "    log_prob_pos = log(stats['p_pos'])\n",
    "    log_prob_neg = log(stats['p_neg'])\n",
    "\n",
    "    log_prob_pos_dict = stats['log_prob_pos_dict']\n",
    "    log_prob_neg_dict = stats['log_prob_neg_dict']\n",
    "\n",
    "    words = tweet.split()\n",
    "    for word in words:\n",
    "        w_log_pos = log_prob_pos_dict.get(word, float('-inf'))\n",
    "        w_log_neg = log_prob_neg_dict.get(word, float('-inf'))\n",
    "\n",
    "        if log_prob_pos != float('-inf'):\n",
    "            log_prob_pos = log_prob_pos + w_log_pos if w_log_pos != float('-inf') else float('-inf')\n",
    "        if log_prob_neg != float('-inf'):\n",
    "            log_prob_neg = log_prob_neg + w_log_neg if w_log_neg != float('-inf') else float('-inf')\n",
    "\n",
    "        if log_prob_pos == float('-inf') and log_prob_neg == float('-inf'):\n",
    "            break\n",
    "\n",
    "    return 1 if log_prob_pos > log_prob_neg else 0\n",
    "\n",
    "def evaluar_modelo_optimizado(test_df, stats):\n",
    "    y_pred = test_df['text'].apply(lambda tweet: predecir_tweet_optimizado(tweet, stats))\n",
    "    y_true = test_df['label'].values\n",
    "    accuracy = np.mean(y_pred == y_true)\n",
    "    return accuracy\n",
    "\n",
    "def k_fold_evaluation(df, k=5):\n",
    "    X = df['text'].values\n",
    "    y = df['label'].values\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    fold_results = []\n",
    "    best_accuracy = -1\n",
    "    best_fold = None\n",
    "\n",
    "    fold_num = 1\n",
    "    for train_index, val_index in skf.split(X, y):\n",
    "        train_df = df.iloc[train_index].reset_index(drop=True)\n",
    "        val_df = df.iloc[val_index].reset_index(drop=True)\n",
    "\n",
    "        stats = construir_diccionario(train_df)\n",
    "        acc = evaluar_modelo_optimizado(val_df, stats)\n",
    "\n",
    "        fold_results.append(acc)\n",
    "        print(f\"Fold {fold_num}, Accuracy: {acc:.4f}\")\n",
    "\n",
    "        if acc > best_accuracy:\n",
    "            best_accuracy = acc\n",
    "            best_fold = fold_num\n",
    "\n",
    "        fold_num += 1\n",
    "\n",
    "    print(f\"\\nResultados K-Fold ({k} folds):\")\n",
    "    print(f\"Mejor fold: {best_fold} con accuracy: {best_accuracy:.4f}\")\n",
    "    print(f\"Accuracy promedio: {np.mean(fold_results):.4f}\")\n",
    "\n",
    "    return best_fold, best_accuracy, fold_results\n",
    "\n",
    "# Main\n",
    "if __name__ == \"__main__\":\n",
    "    ruta_csv = \"./CRI_Practica3/FinalStemmedSentimentAnalysisDataset.csv\"\n",
    "    df = cargar_dataset(ruta_csv, None)\n",
    "    print(\"Ejemplo de datos:\\n\", df.head())\n",
    "\n",
    "    # Ahora usamos k-fold en vez de una sola partición. Al ser un dataset muy grande es conveniente darle un valor a k generoso (se puede variar).\n",
    "    k = 10\n",
    "    best_fold, best_accuracy, fold_results = k_fold_evaluation(df, k=k)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Estrategia 1: Aumentar tamaño de entrenamiento (diccionario variable)\n",
      "Tamaño Train: 20000, accuracy: 0.5915\n",
      "Tamaño Train: 40000, accuracy: 0.5641\n",
      "Tamaño Train: 60000, accuracy: 0.5598\n",
      "Tamaño Train: 80000, accuracy: 0.5528\n",
      "\n",
      "Estrategia 2: Modificar tamaño del diccionario\n",
      "Frecuencia mínima: 1, accuracy: 0.6626\n",
      "Frecuencia mínima: 3, accuracy: 0.6238\n",
      "Frecuencia mínima: 5, accuracy: 0.6075\n",
      "Frecuencia mínima: 10, accuracy: 0.5873\n",
      "\n",
      "Estrategia 3: Modificar tamaño del conjunto de entrenamiento (diccionario fijo)\n",
      "Tamaño Train: 20000, accuracy: 0.5473\n",
      "Tamaño Train: 40000, accuracy: 0.5631\n",
      "Tamaño Train: 60000, accuracy: 0.5724\n",
      "Tamaño Train: 80000, accuracy: 0.5797\n"
     ]
    }
   ],
   "source": [
    "def dividir_train_test(df, test_size=0.2, seed=42):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    train_df, test_df = train_test_split(df, test_size=test_size, random_state=seed, stratify=df['label'])\n",
    "    return train_df.reset_index(drop=True), test_df.reset_index(drop=True)\n",
    "\n",
    "# Estrategia 1: Aumentar el tamaño del conjunto de entrenamiento (sin Laplace en este ejemplo)\n",
    "def estrategia_1(df, incrementos):\n",
    "    print(\"\\nEstrategia 1: Aumentar tamaño de entrenamiento (diccionario variable)\")\n",
    "    resultados = []\n",
    "    for n in incrementos:\n",
    "        # Aquí reaprovechamos las funciones de la Parte C\n",
    "        train_df, test_df = dividir_train_test(df[:n])\n",
    "        stats = construir_diccionario(train_df)  # USANDO la función de la Parte C\n",
    "        acc = evaluar_modelo_optimizado(test_df, stats)  # USANDO la función de la Parte C\n",
    "        resultados.append((n, acc))\n",
    "        print(f\"Tamaño Train: {n}, accuracy: {acc:.4f}\")\n",
    "    return resultados\n",
    "\n",
    "# Estrategia 2: Modificar tamaño del diccionario manteniendo el train fijo\n",
    "def estrategia_2(df, min_freqs):\n",
    "    print(\"\\nEstrategia 2: Modificar tamaño del diccionario\")\n",
    "    resultados = []\n",
    "    train_df, test_df = dividir_train_test(df)\n",
    "    for mf in min_freqs:\n",
    "        # USANDO construir_diccionario y evaluar_modelo_optimizado de la Parte C\n",
    "        stats = construir_diccionario(train_df, min_freq=mf)\n",
    "        acc = evaluar_modelo_optimizado(test_df, stats)\n",
    "        resultados.append((mf, acc))\n",
    "        print(f\"Frecuencia mínima: {mf}, accuracy: {acc:.4f}\")\n",
    "    return resultados\n",
    "\n",
    "# Estrategia 3: Mantener el diccionario fijo y variar el tamaño del conjunto de entrenamiento\n",
    "def estrategia_3(df, train_sizes):\n",
    "    print(\"\\nEstrategia 3: Modificar tamaño del conjunto de entrenamiento (diccionario fijo)\")\n",
    "    train_df, test_df = dividir_train_test(df, test_size=0.2)\n",
    "    # Construir diccionario con el train completo (diccionario fijo)\n",
    "    stats_fijo = construir_diccionario(train_df)\n",
    "    full_vocab = set(stats_fijo['log_prob_pos_dict'].keys()).union(set(stats_fijo['log_prob_neg_dict'].keys()))\n",
    "    vocab_size = stats_fijo['vocab_size']\n",
    "\n",
    "    resultados = []\n",
    "    for size in train_sizes:\n",
    "        train_subset = train_df.iloc[:size].reset_index(drop=True)\n",
    "\n",
    "        # Recalcular frecuencias con vocabulario fijo:\n",
    "        from collections import Counter\n",
    "        pos_texts = train_subset[train_subset['label'] == 1]['text']\n",
    "        neg_texts = train_subset[train_subset['label'] == 0]['text']\n",
    "\n",
    "        word_counts_pos = Counter()\n",
    "        word_counts_neg = Counter()\n",
    "\n",
    "        for text in pos_texts:\n",
    "            words = [w for w in text.split() if w in full_vocab]\n",
    "            word_counts_pos.update(words)\n",
    "        for text in neg_texts:\n",
    "            words = [w for w in text.split() if w in full_vocab]\n",
    "            word_counts_neg.update(words)\n",
    "\n",
    "        total_tweets = len(train_subset)\n",
    "        p_pos = len(pos_texts)/total_tweets if total_tweets > 0 else 0\n",
    "        p_neg = len(neg_texts)/total_tweets if total_tweets > 0 else 0\n",
    "\n",
    "        # Crear stats con diccionario fijo\n",
    "        stats_subset = {\n",
    "            'word_counts_pos': word_counts_pos,\n",
    "            'word_counts_neg': word_counts_neg,\n",
    "            'p_pos': p_pos,\n",
    "            'p_neg': p_neg,\n",
    "            'vocab_size': vocab_size,\n",
    "        }\n",
    "\n",
    "        total_words_pos = sum(word_counts_pos.values())\n",
    "        total_words_neg = sum(word_counts_neg.values())\n",
    "\n",
    "        if total_words_pos > 0:\n",
    "            log_prob_pos_dict = {w: (np.log(count/total_words_pos) if count>0 else float('-inf')) for w, count in word_counts_pos.items()}\n",
    "        else:\n",
    "            # Si no hay palabras, todos -inf\n",
    "            log_prob_pos_dict = {w: float('-inf') for w in full_vocab}\n",
    "\n",
    "        if total_words_neg > 0:\n",
    "            log_prob_neg_dict = {w: (np.log(count/total_words_neg) if count>0 else float('-inf')) for w, count in word_counts_neg.items()}\n",
    "        else:\n",
    "            log_prob_neg_dict = {w: float('-inf') for w in full_vocab}\n",
    "\n",
    "        stats_subset['log_prob_pos_dict'] = log_prob_pos_dict\n",
    "        stats_subset['log_prob_neg_dict'] = log_prob_neg_dict\n",
    "\n",
    "        acc = evaluar_modelo_optimizado(test_df, stats_subset)  # Uso de evaluar_modelo_optimizado de la Parte C\n",
    "        resultados.append((size, acc))\n",
    "        print(f\"Tamaño Train: {size}, accuracy: {acc:.4f}\")\n",
    "    return resultados\n",
    "\n",
    "# Main (Parte B)\n",
    "if __name__ == \"__main__\":\n",
    "    # Reutilizamos cargar_dataset de la Parte C\n",
    "    df = cargar_dataset(\"./CRI_Practica3/FinalStemmedSentimentAnalysisDataset.csv\", None)\n",
    "\n",
    "    # Ahora solo llamamos a las estrategias usando funciones de la Parte C\n",
    "    incrementos = [20000, 40000, 60000, 80000]\n",
    "    estrategia_1(df, incrementos)\n",
    "\n",
    "    min_freqs = [1, 3, 5, 10]\n",
    "    estrategia_2(df, min_freqs)\n",
    "\n",
    "    train_sizes = [20000, 40000, 60000, 80000]\n",
    "    estrategia_3(df, train_sizes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Estrategia 1 - Parte A]: Aumentar tamaño de entrenamiento con Laplace Smoothing\n",
      "Tamaño Train: 20000, accuracy: 0.7522\n",
      "Tamaño Train: 40000, accuracy: 0.7508\n",
      "Tamaño Train: 60000, accuracy: 0.7614\n",
      "Tamaño Train: 80000, accuracy: 0.7616\n",
      "\n",
      "[Estrategia 2 - Parte A]: Variar tamaño del diccionario con Laplace Smoothing (train fijo)\n",
      "Frecuencia mínima: 1, accuracy: 0.7597\n",
      "Frecuencia mínima: 3, accuracy: 0.7514\n",
      "Frecuencia mínima: 5, accuracy: 0.7462\n",
      "\n",
      "[Estrategia 3 - Parte A]: Mantener diccionario fijo y variar tamaño de entrenamiento con Laplace Smoothing\n",
      "Tamaño Train: 20000, accuracy: 0.7406\n",
      "Tamaño Train: 40000, accuracy: 0.7533\n",
      "Tamaño Train: 60000, accuracy: 0.7562\n",
      "Tamaño Train: 80000, accuracy: 0.7597\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from math import log\n",
    "\n",
    "def predecir_tweet(tweet, stats, alpha=1):\n",
    "    log_prob_pos = log(stats['p_pos'])\n",
    "    log_prob_neg = log(stats['p_neg'])\n",
    "    vocab_size = stats['vocab_size']\n",
    "    words = tweet.split()\n",
    "\n",
    "    words_pos = stats['word_counts_pos']\n",
    "    words_neg = stats['word_counts_neg']\n",
    "\n",
    "    total_words_pos = sum(words_pos.values())\n",
    "    total_words_neg = sum(words_neg.values())\n",
    "\n",
    "    for word in words:\n",
    "        p_w_pos = (words_pos.get(word, 0) + alpha) / (total_words_pos + alpha * vocab_size)\n",
    "        p_w_neg = (words_neg.get(word, 0) + alpha) / (total_words_neg + alpha * vocab_size)\n",
    "        log_prob_pos += log(p_w_pos)\n",
    "        log_prob_neg += log(p_w_neg)\n",
    "\n",
    "    return 1 if log_prob_pos > log_prob_neg else 0\n",
    "\n",
    "def evaluar_modelo(test_df, stats, alpha=1):\n",
    "    y_pred = test_df['text'].apply(lambda tweet: predecir_tweet(tweet, stats, alpha=alpha))\n",
    "    y_true = test_df['label'].values\n",
    "    return np.mean(y_pred == y_true)\n",
    "\n",
    "# Estrategia 1 con Laplace Smoothing: Aumentar el tamaño de entrenamiento\n",
    "def estrategia_1_laplace(df, incrementos, alpha=1):\n",
    "    print(\"\\n[Estrategia 1 - Parte A]: Aumentar tamaño de entrenamiento con Laplace Smoothing\")\n",
    "    for n in incrementos:\n",
    "        train_df, test_df = dividir_train_test(df[:n])\n",
    "        stats = construir_diccionario(train_df)\n",
    "        acc = evaluar_modelo(test_df, stats, alpha=alpha)\n",
    "        print(f\"Tamaño Train: {n}, accuracy: {acc:.4f}\")\n",
    "\n",
    "# Estrategia 2 con Laplace Smoothing: Fijar train, variar tamaño del diccionario\n",
    "def estrategia_2_laplace(df, min_freqs, alpha=1):\n",
    "    print(\"\\n[Estrategia 2 - Parte A]: Variar tamaño del diccionario con Laplace Smoothing (train fijo)\")\n",
    "    train_df, test_df = dividir_train_test(df)\n",
    "    for min_freq in min_freqs:\n",
    "        stats = construir_diccionario(train_df, min_freq=min_freq)\n",
    "        acc = evaluar_modelo(test_df, stats, alpha=alpha)\n",
    "        print(f\"Frecuencia mínima: {min_freq}, accuracy: {acc:.4f}\")\n",
    "\n",
    "# Estrategia 3 con Laplace Smoothing: Fijar diccionario, variar tamaño del entrenamiento\n",
    "def estrategia_3_laplace(df, train_sizes, alpha=1):\n",
    "    print(\"\\n[Estrategia 3 - Parte A]: Mantener diccionario fijo y variar tamaño de entrenamiento con Laplace Smoothing\")\n",
    "    # Primero creamos un conjunto de entrenamiento y test fijos\n",
    "    train_full_df, test_df = dividir_train_test(df)\n",
    "    # Creamos un diccionario con todo el train, que será nuestro diccionario base\n",
    "    stats_full = construir_diccionario(train_full_df)\n",
    "    full_vocab = set(stats_full['word_counts_pos'].keys()).union(set(stats_full['word_counts_neg'].keys()))\n",
    "    vocab_size = stats_full['vocab_size']\n",
    "\n",
    "    for size in train_sizes:\n",
    "        train_subset = train_full_df[:size]\n",
    "        # Recalculamos las frecuencias solo considerando las palabras del vocab completo\n",
    "        pos_texts = train_subset[train_subset['label'] == 1]['text']\n",
    "        neg_texts = train_subset[train_subset['label'] == 0]['text']\n",
    "        \n",
    "        word_counts_pos = Counter()\n",
    "        word_counts_neg = Counter()\n",
    "        \n",
    "        for text in pos_texts:\n",
    "            word_counts_pos.update([w for w in text.split() if w in full_vocab])\n",
    "        for text in neg_texts:\n",
    "            word_counts_neg.update([w for w in text.split() if w in full_vocab])\n",
    "        \n",
    "        p_pos = len(pos_texts) / len(train_subset)\n",
    "        p_neg = len(neg_texts) / len(train_subset)\n",
    "        \n",
    "        # Ahora creamos un stats con el vocab_size fijo\n",
    "        stats_subset = {\n",
    "            'word_counts_pos': word_counts_pos,\n",
    "            'word_counts_neg': word_counts_neg,\n",
    "            'p_pos': p_pos,\n",
    "            'p_neg': p_neg,\n",
    "            'vocab_size': vocab_size\n",
    "        }\n",
    "        \n",
    "        acc = evaluar_modelo(test_df, stats_subset, alpha=alpha)\n",
    "        print(f\"Tamaño Train: {size}, accuracy: {acc:.4f}\")\n",
    "\n",
    "# Main para la Parte A\n",
    "if __name__ == \"__main__\":\n",
    "    # Cargamos el dataset (definir cargar_dataset, dividir_train_test, construir_diccionario previamente)\n",
    "    ruta_csv = \"./CRI_Practica3/FinalStemmedSentimentAnalysisDataset.csv\"\n",
    "    df = cargar_dataset(ruta_csv, nrows=100000)\n",
    "    # Parámetros\n",
    "    alpha = 1\n",
    "    # Estrategia 1 con Laplace\n",
    "    estrategia_1_laplace(df, incrementos = [20000, 40000, 60000, 80000], alpha=alpha)\n",
    "\n",
    "    # Estrategia 2 con Laplace\n",
    "    estrategia_2_laplace(df,  min_freqs = [1, 3, 5], alpha=alpha)\n",
    "\n",
    "    # Estrategia 3 con Laplace\n",
    "    estrategia_3_laplace(df, train_sizes = [20000, 40000, 60000, 80000], alpha=alpha)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
